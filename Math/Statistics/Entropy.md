

**אנטרופיה היא מדד של אי הודאות או "ההפתעה" של הממוצעת של משתנה**.

כדי להגיע לחישוב אנטרופיה קודם נחשוב על קונספט אחר. "ההפתעה" של אירוע מסוים. לדוגמא אם היו לי בחבילה 10 קלפים, 7 ירוקים ו3 אדומים. 

קודם כל נחשב את ההסתברות להוציא קלף אדום ונקרא לה P1, אשר תהיה 3/10=0.3.
נחשב גם את ההתסברות להוציא קלף ירוק ונקרא לה P2 אשר תהיה 7/10 = 0.7.

לאחר מכן נחשב את ההפתעה של להוציא קלף אדום מהחבילה באמצעות המשוואה הבאה:

![[surprise.png]]

הבסיס של הlog לא משנה מאוד, מפני שאפשר לשנות אותו בקלות(אפשר לקרוא עוד למטה).

נחשב את ההפתעה להוציא קלף ירוק ואת ההפתעה להוציא קלף אדום:

### S1= LOG(1 / P1) = 1.736
### S2= LOG(1 / P2) = 0.514

כמו שציפינו ההפתעה להוציא קלף אדום - S1 תהיה גדולה יותר מההפתעה להוציא קלף ירוק - S2.

לאחר שחישבנו את ההפתעה , חישוב **האנטרופיה של החבילה** נראה כך:

## E = P1\*S1 + P2\*S2
![[entropy1.png]]

לרוב חישוב האנטרופיה יראה אחרת, מפני שמצמצים את ה log(1 / p) כך שיוצא: log(p)-

כך שהמשוואה הסופית תראה כך:

![[entropy2.png]]

חישוב סופי:

  0.7 \* 0.514 *+E  = 0.3 \* 1.736
E = 0.88

בסיס LOG בחישוב:
בסיס 2 מסמן אנטרופיה שנמדדת באמצעות bits. אפשר גם לחשב בבסיס e ואז האטרופיה תמדד ב nats.

אפשר לקרוא על בסיסים שונים כאן: 
https://stats.stackexchange.com/questions/329756/what-is-the-significance-of-the-log-base-being-2-in-entropy#:~:text=If%20the%20base%20of%20the,will%20be%20measured%20in%20bits.

סרטונים:
https://youtu.be/YtebGVx-Fxw - סרטון טוב להבנה