
ה [[Gradient]] של פונקציה זה בעצם הרשימה של הנגזרות השונות. לפי הגרדיאנט נוכל לראות מהו יהיה ה'צעד' המשמעותי ביותר שנוכל לקחת אם נרצה לרדת הכי מהר בפונקציה.

Gradient descent הוא **אלגוריתם** למציאת נקודת מינימום של פונקציה. 
במקרה של למידת מכונה, הפונקציה שלנו תהיה פונקצית העלות או הטעות של החיזוי שלנו כלומר ה [[Cost Function]].

נרצה כל פעם לקחת צעדים קטנים לפי השיפוע של הפונקציה כדי להגיע לנקודת המינימום.

בכל איטרציה של האלגורתים נשנה את המשקולת 
כך: 
![[Pasted image 20220813183209.png]]
זה נראה מפחיד, אבל הסימן של האפס הוא המשקולת שלנו, אלפא הוא קצב הלמידה. והשאר זה בעצם הנגזרת של הפונקציה כאשר הסימן של אפס הוא המשתנה שלנו.

ככה זה נראה:


![[Pasted image 20220813132319.png]]
![[Pasted image 20220813132348.png]]




סרטונים:
**https://youtu.be/sDv4f4s2SB8 - סרטון ממש טוב, אתה תבין הרבה**
![[Pasted image 20220813183052.png]]
https://www.youtube.com/watch?v=vMh0zPT0tLI&ab_channel=StatQuestwithJoshStarmer

https://www.youtube.com/watch?v=IHZwWFHWa-w&ab_channel=3Blue1Brown
